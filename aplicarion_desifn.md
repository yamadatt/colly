#　要件定義

作りたいのはクローリングとスクレイピングするシステム。

以下のシステムの記事全てを取得して、タイトルと本文を取得したい。
https://yamada-tech-memo.netlify.app/

使用言語：golang
使用するパッケージ：colly (https://go-colly.org/)

---

## 1. 収集対象の範囲（スコープ）

| 項目                | 例・検討ポイント                                                     |
| ----------------- | ------------------------------------------------------------ |
| **エントリポイント★**     | `/posts/*` 配下を対象にする |
| **リンクの辿り方★**      | 巡回はドメイン内のみに限定する|
| **除外パス**          | 画像・PDF など本文でない静的アセットは無視                                      |
| **robots.txt 対応** | Netlify は通常 `/robots.txt` が置けるので必ず確認                         |

---

## 2. 取得データの定義

| フィールド          | 型・例                              |
| -------------- | -------------------------------- |
| `url`          | 記事のパーマリンク                        |
| `title`        | `<h1>` などから抽出                    |
| `body_html`    | `<article>` や `.content` 内部をそのまま |
| `body_plain`   | HTML → 文字列に変換（検索用）               |
| `published_at` | メタデータ (`<time>`) からパース           |
| `tags`         | `.tags a` など（必要なら）               |
| `scraped_at`   | 取得日時（ISO8601）                    |

---

## 3. プロジェクト構造の設計思想

### 基本方針
- **関心の分離**: 各コンポーネントが単一の責任を持つ
- **テスタビリティ**: 各層が独立してテスト可能
- **設定の外部化**: ハードコーディングを避け、YAML設定で制御
- **Collyの特徴活用**: フレームワークの強みを最大限利用

### ディレクトリ構成の考え方

```
colly/
├── cmd/                         # アプリケーションエントリポイント
├── internal/                    # 内部パッケージ（外部からimport不可）
│   ├── collector/              # Colly固有の設定・ハンドラー
│   ├── scraper/                # スクレイピングビジネスロジック
│   ├── storage/                # データ永続化層
│   └── models/                 # データ構造定義
├── pkg/                        # 再利用可能なユーティリティ
├── configs/                    # 設定ファイル
├── data/                       # データ保存・ログ出力
└── test/                       # テストファイル・フィクスチャ
```

### 設計原則
1. **依存関係の方向**: 上位層が下位層に依存（逆依存なし）
2. **インターフェース活用**: ストレージ層等は抽象化
3. **設定駆動**: 動作をYAMLで制御可能
4. **エラーハンドリング**: 各層で適切なエラー処理

---

## 4. 設定ファイル設計思想（YAML形式）

### 設計方針
- **人間可読性**: コメント付きで理解しやすい
- **環境分離**: dev/prod等で設定を分離可能
- **階層構造**: 論理的にグループ化
- **型安全性**: Goの構造体にマッピング

### 設定カテゴリ

#### アプリケーション設定
- アプリ名、バージョン、ログレベル
- 全体的な動作制御

#### 対象サイト設定
- ベースURL、許可ドメイン
- 開始URL、除外パターン
- サイト固有の制約

#### クローラー動作設定
- 並行処理数、リクエスト間隔
- タイムアウト、リトライ戦略
- 丁寧なクローリングの実現

#### HTMLセレクター設定
- 記事抽出用セレクター
- ナビゲーション用セレクター
- サイト構造変更への対応

#### データ保存設定
- 保存形式、出力パス
- バックアップ戦略
- データ管理ポリシー
- **出力形式**: JSONL（JSON Lines）形式を採用

#### ログ・モニタリング設定
- ログレベル、出力先
- ローテーション設定
- 運用監視の考慮

#### パフォーマンス設定
- メモリ制限、バッチサイズ
- キャッシュ戦略
- スケーラビリティ対応

---

## 5. Golang & Colly で必要な設計要素

| 項目               | 具体例                                                                       |
| ---------------- | ------------------------------------------------------------------------- |
| **Collector 設定** | User-Agent, Async/concurrent, MaxDepth                                    |
| **Rate Limit★**  | 1 req/sec など polite crawling を推奨                                          |
| **ページネーションへの対応** | 「…1 2 …18」のようなリンクを `OnHTML("a[href]")` で取得しキューへ追加                         |
| **本文抽出セレクタ★**    | 例：`OnHTML("article")` → `DOM.Text()`<br>レイアウトが変わると壊れるので事前に CSS セレクタを決めておく |
| **エラーハンドリング**    | Retry 回数・失敗 URL ログ出力                                                      |
| **ストレージ層★**      | - ローカル JSON / NDJSON<br>- SQLite / PostgreSQL<br>- S3 などオブジェクトストレージ       |
| **重複検知**         | URL を PK にして Upsert する／ハッシュで比較                                            |
| **テスト方針**        | HTML スナップショットを fixture 化してユニットテスト                                         |

---

## 6. 運用・非機能要件

| 項目              | 決める内容                                                  |
| --------------- | ------------------------------------------------------ |
| **実行形態★**       | ワンショット CLI／Cron で定期実行 |
| **実行頻度★**       | 手動トリガとする                           |
| **ログ & モニタリング** | stdout |
| **デプロイ方法**      | 特になし |
| **スケーラビリティ**    | 記事数が 1 万件になっても対応？ Concurrency 設定を外出しに                  |

---

## 7. セキュリティ・法的考慮

* **robots.txt / 利用規約の遵守**（スクレイピング禁止でないか確認）
* **著作権** – HTML を保存したまま公開しない／引用範囲を検討

---

## 8. 主要コンポーネントの設計思想

### Article データモデル
- **一意性**: URLをプライマリキーとして使用
- **検索性**: プレーンテキスト版も保持
- **追跡性**: 取得日時とハッシュで変更検知
- **拡張性**: タグや著者情報も格納可能

### Collector 初期化
- **ドメイン制限**: 対象サイトのみに限定
- **Rate Limiting**: サイトに負荷をかけない設定
- **設定駆動**: YAML設定から動的に構成
- **ミドルウェア**: 共通処理を分離

### ストレージ抽象化
- **インターフェース設計**: 保存方式を切り替え可能
- **バックアップ対応**: データ損失防止
- **重複処理**: 効率的なデータ管理
- **出力形式**: JSONL形式で1行1記事として保存

---

## 9. 実装優先順位と段階的開発

### Phase 1: 基盤構築
- プロジェクト構造の作成
- YAML設定読み込み機能
- 基本的なCollector設定

### Phase 2: 核心機能
- 単一記事の抽出機能
- HTMLパース処理
- データモデルの実装

### Phase 3: 巡回機能
- リンク巡回ロジック
- データ保存機能
- 重複検知処理

### Phase 4: 品質向上
- エラーハンドリング
- ログ機能
- テストコード

### Phase 5: 運用対応
- パフォーマンス最適化
- モニタリング機能
- ドキュメント整備

---

## 10. データ出力仕様

### JSONL（JSON Lines）形式の採用
- **1行1記事**: 各記事を1行のJSONとして保存
- **ストリーミング対応**: 大量データでもメモリ効率良く処理
- **追記容易**: 新しい記事を末尾に簡単に追加可能
- **ツール対応**: `head`, `tail`, `grep`等のコマンドで操作可能

### ファイル構造例
```
{"url":"https://yamada-tech-memo.netlify.app/posts/article1","title":"記事タイトル1","body_html":"<article>...</article>","body_plain":"記事本文","published_at":"2024-01-01T00:00:00Z","scraped_at":"2024-01-02T10:30:00Z","tags":["tag1","tag2"],"hash":"abc123"}
{"url":"https://yamada-tech-memo.netlify.app/posts/article2","title":"記事タイトル2","body_html":"<article>...</article>","body_plain":"記事本文","published_at":"2024-01-01T00:00:00Z","scraped_at":"2024-01-02T10:30:00Z","tags":["tag3","tag4"],"hash":"def456"}
```

### 改行の扱い
- **ファイルレベル**: 各JSON行は `\n` (LF) で区切り
- **JSON内部**: HTML/テキスト内の改行は自動的に `\\n` でエスケープ
- **文字エンコーディング**: UTF-8で保存

### ファイル命名規則
- **基本**: `articles.jsonl`
- **日付別**: `articles_20240101.jsonl`
- **バックアップ**: `backup/articles_20240101.jsonl.gz`

---

## 11. 技術的な考慮事項

### パフォーマンス
- **メモリ効率**: 大量データ処理時の配慮
- **並行処理**: Collyの非同期機能活用
- **キャッシュ戦略**: 重複処理の回避
- **ストリーミング書き込み**: JSONL形式でメモリ使用量を抑制

### 保守性
- **設定外部化**: コード変更なしでの調整
- **ログ充実**: 問題発生時の調査容易性
- **テスト**: 各コンポーネントの品質保証
- **データ確認**: コマンドラインツールで簡単に内容確認

### 拡張性
- **モジュール設計**: 機能追加時の影響最小化
- **インターフェース**: 実装の差し替え可能性
- **設定階層**: 新しい設定項目の追加容易性
- **フォーマット対応**: 将来的に他形式への出力も可能

---


